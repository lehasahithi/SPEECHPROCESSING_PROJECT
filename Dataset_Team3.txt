Dataset
The dataset used in this study consists of six university lecture videos, each converted into standardized 16 kHz audio recordings to ensure consistency during processing. These recordings were preprocessed through noise reduction and loudness normalization, resulting in clean, high-quality inputs. Using a speech-to-text model, the audio was transcribed into 5,218 time-aligned segments, each enriched with detailed metadata including start/end timestamps, confidence scores, and token probabilities. For every segment, 34 acoustic and temporal features were extracted—such as pitch, energy, jitter, MFCCs, speech rate, and articulation rate—capturing both delivery style and content density. The acoustic features were then integrated with the transcription data to form a comprehensive dataset of shape 5218 × 42, enabling robust analysis, summarization, and note generation.

